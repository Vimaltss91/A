import gitlab
import datetime

# GitLab API credentials and project information
gitlab_url = 'https://gitlab.com'  # Update with your GitLab instance URL
private_token = 'YOUR_PRIVATE_TOKEN'  # Replace with your GitLab private token
project_id = 'your/project'  # Replace with your project ID or path (e.g., 'namespace/project')

# Schedule information
schedule_name = 'Trigger_NB'
release_tag = '23.4.0'

# Create a GitLab API client
gl = gitlab.Gitlab(gitlab_url, private_token=private_token)

# Find the project by ID or path
project = gl.projects.get(project_id)

# Get all CI/CD schedules for the project
schedules = project.schedules.list()

# Find the schedule with the specified name
target_schedule = None
for schedule in schedules:
    if schedule.name == schedule_name:
        target_schedule = schedule
        break

# Update the release_tag parameter in the schedule
if target_schedule:
    target_schedule.variables = [
        {'key': 'release_tag', 'value': release_tag}
    ]

    # Save the changes
    target_schedule.save()
    print(f'Schedule "{schedule_name}" updated successfully.')
else:
    print(f'Schedule "{schedule_name}" not found.')

--------------------------

import requests

# Replace with your GitLab credentials and schedule details
gitlab_url = "https://gitlab.example.com"
private_token = "your_personal_access_token"
project_id = 12345
pipeline_schedule_id = 67890

headers = {
    "Authorization": f"Bearer {private_token}",
    "Content-Type": "application/json"
}

data = {
    "description": "Trigger_NB",  # Optional, update description if needed
    "ref": "release_tag",
    "cron": "0 1 * * *",  # Adjust cron expression as needed
    "cron_timezone": "UTC",  # Specify desired time zone
    "variables": [
        { "key": "release_tag", "value": "23.4.0" }
    ]
}

url = f"{gitlab_url}/api/v4/projects/{project_id}/pipeline_schedules/{pipeline_schedule_id}"

response = requests.put(url, headers=headers, json=data)

if response.status_code == 200:
    print("Schedule updated successfully!")
else:
    print("Error updating schedule:", response.text)




#!/bin/bash

# Set your GitLab access token and project ID
GITLAB_TOKEN="YOUR_ACCESS_TOKEN"
PROJECT_ID="YOUR_PROJECT_ID"
PIPELINE_ID="YOUR_PIPELINE_ID"

# API endpoint to get pipeline details
API_URL="https://gitlab.com/api/v4/projects/$PROJECT_ID/pipelines/$PIPELINE_ID/jobs"

# Function to check if any job is in running state
function check_running_jobs {
  RUNNING_JOBS=$(echo "$1" | jq -r '.[] | select(.status == "running") | .name')
  if [ -n "$RUNNING_JOBS" ]; then
    echo "Waiting for running jobs to complete..."
    sleep 180  # Wait for 3 minutes
  fi
}

# Function to get failed jobs
function get_failed_jobs {
  JOBS=$(curl --header "PRIVATE-TOKEN: $GITLAB_TOKEN" "$API_URL")
  FAILED_JOBS=$(echo "$JOBS" | jq -r '.[] | select(.status == "failed") | .name')
  echo "$FAILED_JOBS"
}

# Check if any job is running and wait if needed
while true; do
  RUNNING_JOBS=$(curl --header "PRIVATE-TOKEN: $GITLAB_TOKEN" "$API_URL" | jq -r '.[] | select(.status == "running")')
  if [ -n "$RUNNING_JOBS" ]; then
    check_running_jobs "$RUNNING_JOBS"
  else
    break
  fi
done

# Get and print the list of failed jobs
FAILED_JOBS=$(get_failed_jobs)
if [ -n "$FAILED_JOBS" ]; then
  echo "Failed jobs in pipeline $PIPELINE_ID:"
  echo "$FAILED_JOBS"
else
  echo "No failed jobs in pipeline $PIPELINE_ID."
fi



#!/bin/bash

# Set your GitLab access token and project ID
GITLAB_TOKEN="YOUR_ACCESS_TOKEN"
PROJECT_ID="YOUR_PROJECT_ID"
PIPELINE_ID="YOUR_PIPELINE_ID"

# API endpoint to get pipeline details
API_URL="https://gitlab.com/api/v4/projects/$PROJECT_ID/pipelines/$PIPELINE_ID/jobs"

# Function to check if any job is in running state
function check_running_jobs {
  RUNNING_JOBS=$(echo "$1" | jq -r '.[] | select(.status == "running") | .name')
  if [ -n "$RUNNING_JOBS" ]; then
    echo "Waiting for running jobs to complete..."
    sleep 180  # Wait for 3 minutes
  fi
}

# Function to retry failed jobs on 'dev', 'staging', or 'master'
function retry_failed_jobs {
  JOBS=$(curl --header "PRIVATE-TOKEN: $GITLAB_TOKEN" "$API_URL")
  FAILED_JOBS=$(echo "$JOBS" | jq -r '.[] | select(.status == "failed" and (.name != "trigger_retry") and (.ref == "dev" or .ref == "staging" or .ref == "master")) | .name')
  
  for job in $FAILED_JOBS; do
    # Retry the job two times using GitLab API
    for i in {1..2}; do
      echo "Retrying $job (attempt $i)..."
      curl --request POST --header "PRIVATE-TOKEN: $GITLAB_TOKEN" "$API_URL/$job/retry"
      sleep 10  # Add a small delay between retries
    done
  done
}

# Check if any job is running and wait if needed
while true; do
  RUNNING_JOBS=$(curl --header "PRIVATE-TOKEN: $GITLAB_TOKEN" "$API_URL" | jq -r '.[] | select(.status == "running")')
  if [ -n "$RUNNING_JOBS" ]; then
    check_running_jobs "$RUNNING_JOBS"
  else
    break
  fi
done

# Retry failed jobs on 'dev', 'staging', or 'master'
retry_failed_jobs



import requests

# GitLab API URL
gitlab_api_url = "https://gitlab.example.com/api/graphql"

# Personal access token for authentication
access_token = "YOUR_ACCESS_TOKEN"

# GraphQL query to get downstream jobs for a specific job
graphql_query = """
query {
  project(fullPath: "namespace/project") {
    job(jobId: "JOB_ID") {
      downstreamPipelines {
        nodes {
          id
          status
          created_at
        }
      }
    }
  }
}
"""

# Replace "namespace/project" with your GitLab project's namespace and name
# Replace "JOB_ID" with the ID of the job you want to query

# Define the HTTP headers with the access token
headers = {
    "Authorization": f"Bearer {access_token}",
}

# Make the GraphQL request
response = requests.post(gitlab_api_url, json={"query": graphql_query}, headers=headers)

# Check if the request was successful
if response.status_code == 200:
    data = response.json()
    # Extract downstream job information from the response
    downstream_jobs = data["data"]["project"]["job"]["downstreamPipelines"]["nodes"]
    for job in downstream_jobs:
        print(f"Job ID: {job['id']}, Status: {job['status']}, Created At: {job['created_at']}")
else:
    print(f"Failed to fetch data. Status code: {response.status_code}")
    print(response.text)



We have implemented auto-scaling in our system, but its efficiency is hindered by the use of the 'safe auto-evict' parameter in all our pods. Consequently, when new nodes are scaled up, the system does not scale down efficiently, even when the overall resource usage is low. This is because pods are distributed across multiple nodes, leading to underutilization of resources on these nodes


"We've implemented auto-scaling, but its efficiency is compromised due to the uniform use of the 'safe auto-evict' parameter across all our pods. Consequently, scaling up new nodes doesn't lead to the scaling down of underutilized nodes, as our pods are distributed across multiple nodes. In response to this, we've already reduced the OCPU from 16 to 8 for worker nodes and plan to further reduce it to 6 this weekend.

To address this issue, we are considering two options:

Develop a script to monitor CPU utilization, enabling the cordoning of nodes. This would terminate new nodes once the associated namespaces are deleted.

Implement the 'safe to evict' parameter selectively, applying it only to necessary pods rather than all pods. This involves identifying the specific pods that require this setting.

We would appreciate your insights on these options and any additional suggestions you may have for effectively scaling down nodes."
