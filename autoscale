kubectl get pod -A -o custom-columns='NAMESPACE:.metadata.namespace,CPU_REQUESTS:.spec.containers[*].resources.requests.cpu' | awk '
BEGIN { print "Namespace\tTotal_CPU_Requests" }
NR > 1 {
  cpu_sum[$1] += ($2 ~ "m" ? $2 + 0 : $2 * 1000)
}
END {
  for (namespace in cpu_sum) {
    print namespace, "\t", cpu_sum[namespace] "m"
  }
}'



Danish, can you test the parallel execution notification using the CI/CD setup?
Additionally, the namespaces are not in the o-devops format, so the auto-cleanup script could not detect them. As a result, it has been running for over 3 days.

Yesterday, on the o-devops-pol16 namespace, the upgrade and rollback for the 15th NB was deployed. During testing, the job runner got stuck. I attempted to re-run the job, but it failed with a 400 BAD REQUEST error.

To address this, I deleted the record from the UI. However, since a .lock file existed, the namespace wasnâ€™t deleted. This resulted in the namespace being marked as available in the database.

This behavior highlights a bug in the auto-namespace deployment script: while the script correctly identifies the .lock file and skips cleanup, the UI does not perform this check, allowing the namespace to be incorrectly marked as available. I will log a bug to address this issue.


What should happen if the required Diam-Sim version is unavailable? Should it fall back to the most recent previous version?

Previously, we utilized the VM.Standard.E3.Flex shape, which worked seamlessly with ATS. However, due to host unavailability issues with the E3 shape, we transitioned to the X7 shape as recommended by SWATGO.

sum(rate(container_cpu_cfs_throttled_seconds_total{container!=""}[5m])) by (pod) > 0


### Auto Namespace Management for Parallel Execution/Deployment

To support parallel execution and deployment, the following enhancements have been made:

---

#### **Database Modifications**
Two new columns are added to the MySQL database to manage parallel deployment configurations:
1. **`parallel_execution`**: Accepts values `Yes` or `No` to indicate if the deployment supports parallel execution.
2. **`parallel_count`**: Holds integer values (`1`, `2`, `3`, etc.) to uniquely identify rows for each parallel deployment instance.

---

#### **Pipeline Parameters**
The **one-click-play pipeline** now includes the following parameters:

1. **`PARALLEL_PKG_TYPE`**: Defines the deployment type. Possible values are:
   - `tgz`: For TGZ package deployments.
   - `csar`: For CSAR package deployments.
   - `asm`: For ASM package deployments.
   - `external`: For external deployments.

2. **Namespace Parameters for Parallel Deployment**:
   - **CSAR Parallel Deployment**:
     - `CSAR_ASM_POLICY_NAMESPACE_1`
     - `CSAR_ASM_POLICY_NAMESPACE_2`
     - `CSAR_ASM_POLICY_NAMESPACE_3`
   - **ASM Parallel Deployment**:
     - `ASM_POLICY_NAMESPACE_1`
     - `ASM_POLICY_NAMESPACE_2`
     - `ASM_POLICY_NAMESPACE_3`
   - **TGZ Parallel Deployment**:
     - `TGZ_POLICY_NAMESPACE_1`
     - `TGZ_POLICY_NAMESPACE_2`
     - `TGZ_POLICY_NAMESPACE_3`
   - **External Parallel Deployment**:
     - `POLICY_NAMESPACE_1`
     - `POLICY_NAMESPACE_2`
     - `POLICY_NAMESPACE_3`

---

#### **Pipeline Behavior**
1. **Triggering Parallel Deployment**:
   - In the `export_downstream_job`, for each parallel deployment, three rows are inserted into the database with identical parameter values.
   - Each row has a unique `parallel_count` value (`1`, `2`, or `3`).

2. **Namespace Assignment**:
   - During deployment, the pipeline fetches the appropriate row for each `parallel_count`.
   - The namespace is assigned sequentially based on the `parallel_count` value.

---

#### **Other Features**
All other features and functionalities remain consistent with the existing deployment process.
