Dynamic Namespace Allocation Approach

Current Challenges:

Inefficient Resource Utilization:

Pre-dev and NB namespaces are reserved even when they are not in use, leading to underutilization of resources. The pre-dev team might not run their pipeline daily, yet the resources remain locked for them. Similarly, after the NB run, the resources are not effectively utilized as they are completely allocated.
Overprovisioning and Lack of Resource Monitoring:

We have increased our DevOps namespace resources, but in reality, we don’t have sufficient resources to support both the DevOps and pre-dev namespaces simultaneously. If all pipelines are triggered at once, there is no mechanism to check if the resource capacity has been maxed out, potentially impacting ongoing ATS runs.
Difficulty in Resource Tracking:

For DevOps, we are not regularly updating the equipment-sharing page, making it difficult to track resource allocation and availability.
Proposed Solution: Automated Namespace Allocation:

Overview: The proposed approach introduces an automated namespace allocation system using a namespace_pool. When a pipeline is triggered, it will check the namespace_pool for available namespaces and allocate one if available. If no namespaces are available, the system will wait until a namespace becomes free.

Detailed Implementation:

The backend utilizes a MySQL database hosted on an OCI bastion to store and manage all relevant information. This database serves as a central repository for all build information when pipelines are triggered.

Database Structure:

Table 1 - namespace_status: Stores pipeline environment variables.

Table 2 - namespace_pool: Manages the namespace pool with the following columns:

namespace_status Table Columns:

nf_type, release_tag, ats_release_tag, namespace, is_csar, is_asm, is_tgz, is_internal_ats, is_occ, is_pcf, is_converged, upg_rollback, official_build, priority, status, ats_status, ats_link, owner, pipeline, cpu_estimate, allocation_lock, custom_message, deployment_Date, date
Key Columns:

official_build: Indicates when the report is true.
priority: Specifies the build priority (Critical, High, Medium, Low) based on the release tag (e.g., Critical for GA builds, High for NB/RC/Beta builds, Medium for pre-dev builds, Low for feature branch builds).
status: Indicates whether the namespace is "YET TO BE ASSIGNED" or "ASSIGNED".
ats_status: Shows whether ATS is "Running" or "Scale Down".
ats_link: Provides the ATS Jenkins link.
cpu_estimate: Used to estimate the CPU requirements (e.g., policy at 80 CPUs, BSF at 50 CPUs).
allocation_lock: Prevents collisions when running parallel pipelines.
namespace_pool Table Columns:

namespace: Name of the namespace.
status: Indicates whether the namespace is "Available" or "In-Use".
Pipeline Execution Process:

When a pipeline is triggered for build and deployment, the downstream pipeline (export_downstream_test_{nf}_variable) will first fetch and insert relevant data into the namespace_status table. If the same data already exists, it won’t be added again.

In the initial stage of the build, the namespace column is blank, and the status is marked as "YET TO BE ASSIGNED". The build job will continue with the usual processes (e.g., jar, common-svc, cnpolicy, bsf, ats).

During the load_bastion job, the system will query the data based on specific parameters (e.g., nf_type, release_tag, ats_release_tag, etc.) and check the following three conditions:

CPU Availability:

It will retrieve the CPU requests from Prometheus and verify if the total CPU usage is below the CPU limit (defined as CPU_LIMIT_HIGH = Max_Total_CPU - 200).
Parallel Deployment Consideration:

When multiple deployments run in parallel, the estimate_cpu value becomes crucial as Prometheus may not account for those CPU counts. The system ensures that the sum of total_cpu_request and estimate_cpu_request remains below the CPU_LIMIT_HIGH.
Priority Management:

The system checks for any high-priority jobs in the database within the last 1 to 1.5 hours. If such jobs exist and their estimate_cpu does not exceed CPU_LIMIT_HIGH, the system proceeds with the deployment. If the estimate surpasses the limit, the system waits for the high-priority job to complete before allocating resources.
If these conditions are not met, the system will wait for 10 minutes and recheck. Once sufficient resources are available, the namespace will be assigned to the current job, and the namespace_pool will mark it as "In-Use".

Post-ATS Process: Once the ATS process is complete, the results are posted in Slack. During namespace deletion, the specific namespace is deleted, making it available in the pool again. If the ATS is scaled down, the ats_status in the database will be updated from "Running" to "Scale Down". Note that manual intervention is required to delete the namespace post-debugging, ensuring it is available for other jobs.

Enhanced Visibility: To address visibility issues, a frontend UI has been created using Python (Django) as the backend, connecting to the database. This UI provides interactive options for better management and visibility of resources and namespaces.


In Python, some packages are dependent on specific versions of the interpreter. For example, if a package is built for Python 3.9 or 3.11, it might not be compatible with Python 3.12 without being rebuilt. This is the case with the bcrypt package, which is available on PyPI for Python 3.9 and 3.11. Since you're using Python 3.12, the package is attempting to build at runtime, but it seems to be failing due to missing dependencies.


#!/bin/bash

# Variables
namespace="your_namespace_value"  # Replace with your namespace value
ATS_link="your_ATS_link_value"    # Replace with your ATS link value

# MySQL command to update the ats_status and ats_link
mysql -u your_username -p'your_password' -h your_host -D your_database -e "
UPDATE namespace_status 
SET ats_status = 'Running', ats_link = '${ATS_link}'
WHERE namespace = '${namespace}';
"


New Tables
-----------------

CREATE TABLE namespace_status (
    s_no INT AUTO_INCREMENT PRIMARY KEY,
    nf_type VARCHAR(20),
    release_tag VARCHAR(30),
    ats_release_tag VARCHAR(30),
    namespace VARCHAR(50),
    is_csar ENUM('YES','NO') NOT NULL,
    is_asm ENUM('YES','NO') NOT NULL,
    is_tgz ENUM('YES','NO') NOT NULL,
    is_internal_ats ENUM('YES','NO') NOT NULL,
    is_occ ENUM('YES','NO') NOT NULL,
    is_pcf ENUM('YES','NO') NOT NULL,
	is_converged ENUM('YES','NO') NOT NULL,
    upg_rollback ENUM('YES','NO') NOT NULL,
    official_build ENUM('YES','NO') NOT NULL,
    priority ENUM('Critical','High','Medium','Low') NOT NULL,
    status VARCHAR(50),
    ats_status VARCHAR(50),
    ats_link VARCHAR(100),
    owner VARCHAR(50),
    pipeline VARCHAR(100),
    cpu_estimate VARCHAR(20),
    allocation_lock ENUM('YES','NO') NOT NULL,
    custom_message VARCHAR(200),
    deployment_Date TIMESTAMP,
    date TIMESTAMP DEFAULT CURRENT_TIMESTAMP 
);


INSERT INTO namespace_status (nf_type, release_tag,ats_release_tag, namespace, is_csar, is_asm, is_tgz, is_internal_ats, is_occ, is_pcf, is_converged, upg_rollback , official_build, priority , status, ats_status, ats_link, owner, pipeline, cpu_estimate, allocation_lock) VALUES
('policy','24.3.0-OCNGF-34343','24.3.0-OCNGF-34343', '', 'NO', 'NO', 'NO', 'YES', 'NO', 'NO', 'YES', 'NO','YES', 'High','YET TO ASSIGN','','','vimal','','60'  ,'NO','@vsanthar @edugar fyi for the debug message','' );

INSERT INTO namespace_status (
    nf_type,
    release_tag,
    ats_release_tag,
    namespace,
    is_csar,
    is_asm,
    is_tgz,
    is_internal_ats,
    is_occ,
    is_pcf,
    is_converged,
    upg_rollback,
    official_build,
    priority,
    status,
    ats_status,
    ats_link,
    owner,
    pipeline,
    cpu_estimate,
    allocation_lock,
    custom_message,
    deployment_Date
) VALUES
(
    'bsf',
    '23.4.0-ocngf-4342',
    '23.4.0-ocngf-4342',
    '',
    'YES',
    'NO',
    'YES',
    'NO',
    'YES',
    'NO',
    'YES',
    'NO',
    'YES',
    'High',
    '',
    '',
    '',
    'Alice',
    '',
    '2',
    'NO',
    '@vsanthar @edugar fyi for the debug message',
    NULL
);




import requests
import json

def fetch_cpu_requests(prometheus_url):
    """
    Fetches CPU requests data from Prometheus.

    Parameters:
    prometheus_url (str): The URL of the Prometheus server.

    Returns:
    dict: The JSON response from Prometheus containing the query results.
    """

    # Prometheus query for CPU requests
    query = 'sum(kube_pod_container_resource_requests_cpu_cores) by (namespace, pod, container)'

    # Define the URL for the query API
    api_url = f"{prometheus_url}/api/v1/query"

    # Parameters to be sent to the API
    params = {
        'query': query
    }

    try:
        # Send the HTTP GET request
        response = requests.get(api_url, params=params)

        # Raise an exception for HTTP errors
        response.raise_for_status()

        # Parse the JSON response
        data = response.json()

        # Check for Prometheus API errors
        if data.get('status') != 'success':
            print("Error in Prometheus query:", data.get('error'))
            return None

        # Return the data part of the response
        return data['data']['result']

    except requests.exceptions.RequestException as e:
        print(f"Error fetching data from Prometheus: {e}")
        return None

# Example usage
prometheus_url = 'http://localhost:9090'
cpu_requests_data = fetch_cpu_requests(prometheus_url)

if cpu_requests_data:
    print(json.dumps(cpu_requests_data, indent=4))
