In Python, pip dependency resolution doesn't work the same way as in Java. When specifying both an open-source repository (primary) and a secondary repository (e.g., repo1), pip defaults to using the secondary repository (repo1) if it contains all required dependencies. As a result, all dependencies are retrieved from repo1 instead of the primary open-source repository.

# Generate SHA-256 checksum file with only the filename in the checksum output
    filename = os.path.basename(file_path)
    sha256_file_path = f"{remote_path}.sha256"
    stdin, stdout, stderr = ssh.exec_command(
        f"cd {vm_path} && sha256sum {filename} > {filename}.sha256"
    )
    stdout.channel.recv_exit_status()
    print(f"SHA-256 checksum file created at {vm_path}/{filename}.sha256 on {vm_ip}")


# Generate SHA-256 checksum file on the remote VM
    sha256_file_path = f"{remote_path}.sha256"
    stdin, stdout, stderr = ssh.exec_command(f"sha256sum {remote_path} > {sha256_file_path}")
    stdout.channel.recv_exit_status()
    print(f"SHA-256 checksum file created at {sha256_file_path} on {vm_ip}")


def scp_package_to_vm(file_path, vm_ip, vm_path, username, password):
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(vm_ip, username=username, password=password)

    # Create the target directory and set permissions
    stdin, stdout, stderr = ssh.exec_command(f"mkdir -p {vm_path} && chmod 777 {vm_path}")
    stdout.channel.recv_exit_status()

    # SCP the file using SFTP
    with ssh.open_sftp() as sftp:  # Open SFTP session directly on the ssh client
        remote_path = os.path.join(vm_path, os.path.basename(file_path))
        sftp.put(file_path, remote_path)
        print(f"Package sent to {vm_ip}:{remote_path}")

    # Close SSH connection
    ssh.close()



def scp_package_to_vm(file_path, vm_ip, vm_path, username, password):
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(vm_ip, username=username, password=password)

    # Create the target directory and set permissions
    stdin, stdout, stderr = ssh.exec_command(f"mkdir -p {vm_path} && chmod 777 {vm_path}")
    stdout.channel.recv_exit_status()
    
    # SCP the file
    with paramiko.Transport((vm_ip, 22)) as transport:
        transport.connect(username=username, password=password)
        sftp = transport.open_sftp()  # Corrected from open_file to open_sftp
        sftp.put(file_path, os.path.join(vm_path, os.path.basename(file_path)))
        sftp.close()
    print(f"Package sent to {vm_ip}:{vm_path}")




import os
import requests
import paramiko
import subprocess
from zipfile import ZipFile

# Base URLs
SOURCE_BASE_URL = "https://artifactoryhub.com/artifactory/db"
TARGET_BASE_URL = "https://artifactoryhub.com/artifactory/dbtier"

# Get environment variables
DB_TIER_TAG = os.getenv("DB_TIER_TAG")
VM1_USER = os.getenv("LOCALVM1_USER", "root")
VM1_PASS = os.getenv("LOCALVM1_PASS", "root")
VM1_IP = os.getenv("LOCALVM1_IP")
VM2_USER = os.getenv("LOCALVM2_USER", "root")
VM2_PASS = os.getenv("LOCALVM2_PASS", "root")
VM2_IP = os.getenv("LOCALVM2_IP")

# Generate package name and URLs
version_parts = DB_TIER_TAG.split('-')  # Split into version and tag parts if present
version_base = version_parts[0].replace('.', '_')  # Convert '24.3.0' to '24_3_0'
suffix = f"-{version_parts[1]}" if len(version_parts) > 1 else ""  # Add suffix if it exists

package_name = f"dbtier_{version_base}_0_0_0{suffix}.zip"
download_url = f"{SOURCE_BASE_URL}/{DB_TIER_TAG}/{package_name}"
upload_url = f"{TARGET_BASE_URL}/{DB_TIER_TAG}/{package_name}"
local_path = f"/home/{package_name}"

# Step 1: Download the DB package
def download_package(url, download_path):
    response = requests.get(url, stream=True)
    if response.status_code == 200:
        with open(download_path, 'wb') as file:
            for chunk in response.iter_content(1024):
                file.write(chunk)
        print(f"Downloaded {download_path}")
    else:
        print("Failed to download package.")

# Step 2(i): Push the package to a different location in Artifactory
def upload_package_to_artifactory(file_path, upload_url, username, api_key):
    with open(file_path, 'rb') as file:
        response = requests.put(upload_url, auth=(username, api_key), data=file)
        if response.status_code == 201:
            print("Uploaded package to Artifactory successfully.")
        else:
            print("Failed to upload package to Artifactory.")

# Step 2(ii) & 2(iii): SCP the package to VMs
def scp_package_to_vm(file_path, vm_ip, vm_path, username, password):
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(vm_ip, username=username, password=password)

    # Create the target directory and set permissions
    stdin, stdout, stderr = ssh.exec_command(f"mkdir -p {vm_path} && chmod 777 {vm_path}")
    stdout.channel.recv_exit_status()
    
    # SCP the file
    with paramiko.Transport((vm_ip, 22)) as transport:
        transport.connect(username=username, password=password)
        sftp = transport.open_sftp()
        sftp.put(file_path, os.path.join(vm_path, os.path.basename(file_path)))
        sftp.close()
    print(f"Package sent to {vm_ip}:{vm_path}")

# Step 3(i): Unzip and load Docker images
def load_docker_images(zip_path):
    with ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall('/tmp/db_package')
    image_path = '/tmp/db_package/Artifacts/Images'
    for image_file in os.listdir(image_path):
        if image_file.endswith('.tar'):
            file_path = os.path.join(image_path, image_file)
            subprocess.run(['docker', 'load', '-i', file_path])

# Step 3(ii): Tag and push Docker images
def tag_and_push_images():
    # List all loaded images
    result = subprocess.run(['docker', 'images', '--format', '{{.Repository}}:{{.Tag}}'], capture_output=True, text=True)
    images = result.stdout.splitlines()
    
    for image in images:
        new_tag = f"cgbu-docker-dev/db/{image.split('/')[-1]}"
        subprocess.run(['docker', 'tag', image, new_tag])
        subprocess.run(['docker', 'push', new_tag])
        print(f"Tagged and pushed {new_tag}")

# Define main function
def main():
    artifactory_user = "mail@APIKEY"
    artifactory_api_key = "YOUR_API_KEY"

    # Step 1
    download_package(download_url, local_path)

    # Step 2
    upload_package_to_artifactory(local_path, upload_url, artifactory_user, artifactory_api_key)
    
    # SCP to VMs
    vm1_path = f"/var/www/html/builds/DBtier/{DB_TIER_TAG}"
    vm2_path = f"/var/www/html/builds/DBtier/{DB_TIER_TAG}"
    scp_package_to_vm(local_path, VM1_IP, vm1_path, VM1_USER, VM1_PASS)
    scp_package_to_vm(local_path, VM2_IP, vm2_path, VM2_USER, VM2_PASS)

    # Step 3
    load_docker_images(local_path)
    tag_and_push_images()

if __name__ == "__main__":
    main()
